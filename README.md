### Alignment
## Тестовое задание

**RLHF** [1] — фундаментальный метод алаймента языковых моделей, который применялся при обучении ChatGPT, LLama 2, Qwen 2.5 и т.д. Как известно RLHF очень требователен к ресурсам и чувствителен к гиперпараметрам. Сложность данного метода обоснована сложностью алгоритма PPO [2], который лежит в основе RLHF.

Статья **Back to Basics** [3] предлагает использовать более простой алгоритм REINFORCE для алаймента языковых моделей. Согласно результатам данной работы, данный алгоритм не только проще в реализации, но и показывает лучшие метрики, чем PPO. 

## Level 1

Мы предлагаем вам реализовать алгоритм REINFORCE w/ baseline для алаймента языковых моделей. Обращаем ваше внимание, что вам не нужно реализовывать RLOO для подсчёта baseline, достаточно взять moving average.

- Используйте [HuggingFaceTB/SmolLM2-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) в качестве SFT модели.
- Поверх SFT обучите Reward Model на парах из датасета [esfrankel17/HelpSteer2_binarized](https://huggingface.co/datasets/esfrankel17/HelpSteer2_binarized). Разбейте average_rting_split на train и validation подвыборки. На данном этапе рекомендуется использовать RewardTrainer из. библиотеки [trl](https://github.com/huggingface/trl). Достаточно обучить одну эпоху. Используйте learning rate = 5e-5, fp16 и ограничьте максимальную длину так, чтобы избежать Out of Memory.
- Рекомендуем куда-нибудь сохранить полученную RM.
- Реализуйте алгоритм REINFORCE w/ baseline из статьи, используя SFT и RM, полученные на предыдущих шагах. На данном этапе НЕ рекомендуется использовать RLOOTrainer из. библиотеки [trl](https://github.com/huggingface/trl), так как реализация в нём отличается от алгоритма, описанного в статье (**Bonus**: в отчёте опишите почему именно). Используйте batch size и количество итераций, которые позволяют ваши ресурсы. В качестве baseline используйте moving average.
- Выросла ли средняя награда на отложенной выборке (validation split) по сравнению c SFT моделью?

## Level 2

Предположим, что мы хотим обучить Reward Model, которая выдаёт не скалярную оценку, а распределение вероятности поверх дискретных оценок. Пусть оценка текста — натуральное число от 1 до 10, тогда RM выдаёт 10 чисел, каждое из которых — вероятность текста получить соответствующую оценку (соответственно сумма этих значений равна 1).

- Подумайте, как должна выглядеть функция потерь для обучения такой модели наград, если  мы по-прежнему хотим максимизировать вероятность $p(y_w \succ y_l | x)$.
- Обучите Reward Model с полученной функцией потерь на том же датасете пар.
- Придумайте, как данную модель интегрировать в алгоритм REINFORCE. Какую дополнительную информацию мы можем использовать, если работаем с распределением над оценками? Как этот сигнал интерпретируется?
- Обучите REINFORCE поверх SFT с вероятностной RM. Используйте те же гиперпараметры, что использовали при обучении в Level 1.
- Получилось ли улучшить качество алаймента? Как вы объясните полученный результат?

## Замечание 
- Я использовал датасет [trl-lib/ultrafeedback_binarized](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized), потому что [esfrankel17/HelpSteer2_binarized] уже не существовало на hugginngface, а его аналоги мне не понравились.
- Код каждого уровня реализовывал в отдельных ноутбуках (они лежат в папках Level1 и Level2), потому что я работал colab. Если бы я писал код на отдельном сервере, то мне было бы удобнее писать скрипты и модули.
- В ноутбуках содержится информация только о коде, а все рассуждения и формулы будут написаны в README каждой папки.