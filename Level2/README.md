### Level2
В этом уровне мне нужно было реализовать алгоритм REINFORCE, в котором reward модель предсказывает не скаляр, а вероятностное распределние. Для этого мне нужно было написать свою reward модель и её loss. Также нужно понять, как можно использовать новый формат reward.
## Reward model
Написать код модели тривиально, нужно просто к sft моделе добавить один последний линейный слой с размером выхода 10. 
Основная задача состоит в том, чтобы написать корректную loss функцию для обучения модели. Для этого посмотрим на loss функции стандартной reward модели (Bradley-Terry):
\[
p ( y _ { 1 } > y _ { 2 } | x ) = \sigma ( r _ { 1 } - r _ { 2 } )
\]
$r _ { 1 }$ - reward rejected выхода и промпта
$r _ { 2 }$ - reward chosen выхода и промпта
$r _ { 1 }$ и $r _ { 2 }$ это просто скаляры (выходы reward модели). 
Основная идея этой loss функции проста: "награждать" модель за то, что у chosen оценка значительно выше чем у rejected.
# Гипотеза 1.1
Первая гипотеза заключается в том, чтобы просто заменить $r _ { 1 }$ и $r _ { 2 }$ на матожидание от полученного распределения. Мы также будем "поощрять" за максимальное различие между оценками.
# Результат Гипотезы 1.1
Модель стала отличать хорошие выходы и промпты и плохие. Но есть существенный минус: выходные распределения имеют низкую дисперсию, поскольку мой loss подталкивал модель предсказывать только одно число и текущая reward модель становиться мало отличимой от стандартной.
Из этого следует интересная гипотеза, что самое главное преимущество такой reward модели заключается как раз таки в значении дисперсии. Потому что эта информация показывает ничто иное, как уверенность модели в своей оценке. Запомним эту мысль и вернемся к ней ниже.
# Гипотеза 1.2
Что если использовать KL-дивергенцию как меру, того, что предсказанное распределение сильно стало отличаться от label?
В датасете [esfrankel17/HelpSteer2_binarized](https://huggingface.co/datasets/esfrankel17/HelpSteer2_binarized) есть параметры score_chosen и score_rejected, что является label оценкой модели. Попробуем использовать эти значения для построения label распределения. Будем использовать обычное гауссовоское.
Тогда разделим loss функцию на две части pref и score, где pref будет отвечать, насколько корректно модель определяет, какой ответ лучше, а score, то насколько ответы соответствуют действительности.
Функция loss pref будет знакомой
\[
loss_pref ( y _ { 1 } > y _ { 2 } | x ) = \log \left \sigma ( E _ { 2 } - E _ { 1 } )
\]
А функция loss score будет выглядеть:
\[
loss_pref ( y _ { 1 } > y _ { 2 } | x ) = (( D _ kl _ { 2 } - D _ kl _ { 1 } ))/2
\]
Что будет просто средней KL-дивергенцией на данных.
# Результат Гипотезы 1.2
Теперь модель предсказывает уже рапспределение, а не "пики" как было в гипотезе 1.1, но и в этой гипотезе есть недостаток, так как диспрерсия этих распределений очень схожа. Но во всяком случае, она есть, поэтому предлагаю перейти к REINFORCE алгоритму.
## REINFORCE
Главное отличие этого блока от схожего из Level1 заключается в том, чтобы понять, как использовать факт того, что у reward теперь это вероятностное распределение. 
Как уже говорилось выше самое сушественное отличие от классического reward состоит в том, что теперь мы можем понять, когда reward модель не уверена в своей оценке, что можно понять по дисперсии распределения. Если она высокая, то это означает, что модель не уверена в своей оценке и кажется, что брать просто матожидание от распределения будет неправильно, поскольку это оценка будет неточная. 
Также нужно понять, что делать с общей наградой модели при высокой дисперсии. Мне кажется модель нужно штрафовать, так как она выдает неоднозначный текст в любом случае, даже если матожидание высокое. 
# Гипотеза 2.1
Можно накладывать штраф на reward при высокой диспрерсии.
Вариант хороший, но он сильно "ругает" модель, так как в формуле награды (расписана в Level1) из reward вычитается KL-дивергенция, что будет приводить к непредсказуемым повидениям градиентов. Но ведь может быть случай, когда изначальной промпт был некорректным из-за чего модель и выдала неоднозначный ответ. Ещё в этом случае, мы слишком сильно будем приближаться к исходной политики.
# Гипотеза 2.2 
Второй вариант это штрафовать и reward и KL-дивергенцию, поскольку нам нужно штрафовать модель, но мы не можем это делать слишком решительно, потому что ситуация неоднозначная. Я предлагаю делить общую награду среднеквадратичное отклонение + 1, это будет означать, что если модель абсолютно уверенный в своем ответе, то будем "судить" её только по скаляру этого ответа, а если нет, то её будем уменьшать её награду, но не настолько критично, чтобы loss не колебался.
## Результаты
Я обучал на абсолютно таких же параметрах эту модель, что и в Level1.
Результаты выглядят вот так:
```
Evaluating SFT model...
Reward statistics:
Min: 0.66
Max: 0.67
Mean: 0.66

Evaluating trained model...
Reward statistics:
Min: 0.60
Max: 0.72
Mean: 0.66
SFT Reward: 0.659
Trained Reward: 0.662
```
Из результатов можем сразу заметить один существенный недостаток моего метода. Reward модель слишком сильно осторожничает и все её оценки колеблятся от 0.6 до 0.72, а оценки у sft модели вообще неинформативные. Но тем не менее прирост в точности всё же есть.
Почему значение оценок от 0.6 До 0.72? Это скорее всего связано с тем, что я брал в loss функции KL-дивергенцию между предсказываемым распределением и label распределение, а в исходном датасете все оценки как раз таки находится в этом диапазоне, поэтому reward модель превыкла, что оценки могут быть только такими.