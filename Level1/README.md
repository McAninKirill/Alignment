### Level1
В этом уровне мне нужно реализовать алгоритм REINFORCE из статьи [Ahmadian et al, Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs]. Для этого нужна sft модель и reward модель. 
# SFT model 
Я использовал [HuggingFaceTB/SmolLM2-135M-Instruct](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct), как рекомендуется в задании.
# Reward model
Я обучил reward модель поверх sft, используя библиотеку [trl](https://github.com/huggingface/trl). В tokenizer я ограничил max_length значением 512, потому что у меня не хватает вычислительных мощностей.
## REINFORCE
На данном этапе нужно я реализовал формулу из статьи, чтобы оптимизировать sft модель, для более точного соответствую промпта и ответа:
\[
\begin{align*}
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(\cdot|x)} \left[ (R(y, x) - b) \nabla_{\theta} \log \pi_{\theta}(y|x) \right]
\end{align*}
\] (1)
# Награда модели:
$R (x, y)$ - это награда модели, которая вычисляется по формуле:
\[
R(x, y) = r_{\phi}(x, y) - \beta \log \left( \frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y|x)} \right)
\] (2)
$r_{\phi}(x, y)$ - это обращение к reward моделе, чтобы она оценила, насколько выход из current policy соответствует желанию промпта.
$\log \left( \frac{\pi_{\theta}(y|x)}{\pi_{\mathrm{ref}}(y|x)} \right)$ - это ничто иное, как KL-дивергенция между current policy и sft.
$\beta$ - константа для контроля дистанции от current policy
Таким образом, награда должна балансировать между тем, чтобы "хвалить" модель за то, что она выдает качественный ответ, он "ругать" за отклонения от первоначальной модели.
# Baseline
b - это baseline в формуле (1), который нужен для снижение дисперсии оценки, поэтому нужно, чтобы baseline имел высокую ковариацию с оценкой стохастического градиента.
Статься предлогает использовать две опция для снижения дисперсии оценки: Leave-One-Out и moving average. 
Leave-One-Out заключается в том, чтобы в том, чтобы генерировать несколько выборок, для одного промпта. Этот метод мне не подходит, так как каджая генерация стоит очень дорого по времени, а мои ресурсы ограничены.
Гораздо лучше выглядит moving average, который является ничем яным средним от всех наград модели.
\[
b_{\mathrm{MA}} = \frac{1}{S} \sum_{s} R\left(x^s, y^s\right)
\]
## Результаты
Я обучал только на 12 итерации, так как больше по времени не смог. Так как итераций было мало, мне пришлось увеличить learning rate до 1e-3, в иных случаях модель обучалась, но незначительно.
И того результаты обученной модели выглядит вот так:

Evaluating SFT model...
Reward statistics:
Min: 0.03
Max: 0.79
Mean: 0.33

Evaluating trained model...
Reward statistics:
Min: 0.19
Max: 0.72
Mean: 0.43
SFT Reward: 0.325
Trained Reward: 0.434

Прекрасный результат, который показывает сильное повышение качества ответов модели всего на 12 итераций. Самое интересное, что судя по оценкам модель стала значительно реже выдавать откровенно плохие ответы.
